# OLMP

## 神经网络

输入$x_{1},x_{2},x_{3}$

指的是输入到输出的映射关系，映射输入，通过激活函数，得到输出

### SNN

添加了少量隐层的浅层神经网络

### DNN

隐层很多的神经网络

### 应用

手写字符识别

读到一张图片之后，通过神经网络一层一层的计算，算出概率最大的那个字

输入：28*28的图片（每一个样本的特征为784）

​	如果样本数为100，输入为100 * 784

输出：图片对应一个数字

方法：将图片和数字产生联系

第一层：784个结点

第二层的结果为各个节点的线性变换加上一个偏移量

- 如果第二层有300个结点，则可以得到300个不同的值（信号）

对于第三层，同第二层的变换，变成了新的信号

最终输出的是10维的向量，就是图片跟结点之间的概率

每一个数字代表一个颜色

神经网络的训练就是学习如何控制权重，即根据结果让机器来调参

## 神经网络模型的存储

存储的是每个层之间各个参数的权重$w$

存储占用的大小和权重矩阵大小$w$相关

这些数据占用的内存很大，需要压缩

## DNN压缩

Magnitude-based pruning（MP）：给定一个门限值，如果低于这个门限值，就把参数设为0，然后不存储

LMP：给每一层分别设置一个门限值

问题：会影响准确度

- 剪完之后需要重新训练，查看是否回到原来的精度

问题：通常手动调参，很麻烦

在损失精度epsion范围内，找到门限值，使得足够优化

- 这个问题不可微，采用NCS算法进行优化
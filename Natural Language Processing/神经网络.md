# 神经网络

## 前言

经典的神经网络如Fig.1，这个神经网络包含三个层次，输入层，隐藏层和输出层

<img src="https://images2015.cnblogs.com/blog/673793/201512/673793-20151219151604318-1557737289.jpg" alt="img" style="zoom: 50%;" />

[^Fig.1]: 

1. 设计神经网络时，输入层和输出层的节点数往往是固定的，中间层可以任意指定
2. 结构图中的箭头代表着预测过程中数据的流向，和训练时的数据流有一定的区别
3. 结构图中的关键不是圆圈（神经元），而是连接线（神经元之间的连接）。每个连接线对应不同的权重，这个通过训练得到

## 神经元模型

神经元模型是一个包含输入、输出与计算功能的模型，输入可类比为神经元的树突，输出可类比为神经元的轴突，计算可类比为细胞核

Fig.2展示的是典型的神经元：包含3个输入、1个输出和2个计算功能

<img src="https://images2015.cnblogs.com/blog/673793/201512/673793-20151219153856802-307732621.jpg" alt="img" style="zoom:67%;" />

[^Fig.2]: 

神经网络的训练算法就是让权重的值调整到最佳，使得整个网络的预测效果最好

神经网络中，每个有向箭头代表值得加权传递

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230201441792-1505283920.jpg)

[^Fig.3]: 

Fig.3中的输出函数$z=g\left(a_{1}^{*} w_{1}+a_{2}^{*} w_{2}+a_{3}^{*} w_{3}\right)$，其中g就是sgn函数

扩展：

- 将sum和sgn合并到一个圆圈里
- 将输出扩展成多个，但是输出的值都一样

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204036479-461440948.jpg)

[^Fig.4]: 

### 应用

有一个样本，样本中有四个属性，其中三个属性已知，一个属性未知。我们需要的是通过三个已知属性预测未知属性

假设已知的三个属性为$a_{1}$,$a_{2}$,$a_{3}$，已知的属性为$z$

这里将已知得属性称为特征，未知的属性称为目标。假设特征和目标之间是线性关系，且$z$可以被公式算出来，则可以通过神经元模型预测新样本的目标

## 感知器模型

<img src="https://images2015.cnblogs.com/blog/673793/201512/673793-20151230205437995-673856644.jpg" alt="img" style="zoom: 50%;" />

[^Fig.5]: 单层神经网络

当有多个输入输出端口时，单个的数字就变成了矩阵

如Fig.5，输入的变量为$[a_{1},a_{2},a_{3}]^{T}$，用向量${\rm{\vec a}}$表示

输出的结果为$[z_{1},z_{2}]^{T}$，用向量${\rm{\vec z}}$表示

系数就是矩阵${\rm{W}}$，即2行3列的矩阵$\left( {\begin{array}{*{20}{c}}
{\begin{array}{*{20}{c}}
{{w_{1,1}}}\\
{{w_{2,1}}}
\end{array}}&{\begin{array}{*{20}{c}}
{{w_{1,2}}}\\
{{w_{2,2}}}
\end{array}}&{\begin{array}{*{20}{c}}
{{w_{1,3}}}\\
{{w_{2,3}}}
\end{array}}
\end{array}} \right)$

于是，输出公式可改写为$g(\rm{W}\rm{a}) = \rm{z}$

### 应用

用于线性分类任务

## 两层神经网络（多层感知器）

可以解决异或问题，也具有非常好的非线性分类效果

两层神经网络添加了一个中间层，就是有两层计算层

偏置结点（bias unity）

- 只含有存储功能，存储值永远为1的单元

<img src="https://images2015.cnblogs.com/blog/673793/201512/673793-20151226111144687-604911384.jpg" alt="img" style="zoom:50%;" />

[^Fig.6]: 

对于偏置节点，有些神经网络不会明显的画出来，其矩阵运算如下
$$
g\left(\mathbf{W}(1)^{\star} \mathbf{a}(1)+\mathbf{b}(1)\right)=\mathbf{a}(2)
$$

$$
g\left(\mathbf{W}(2)^{*} \mathbf{a}(2)+\mathbf{b}(2)\right)=\mathbf{z}
$$

如果使用平滑函数sigmoid作为函数g，此时g也叫激活函数（active function）

- 用来拟合特征与目标之间真实函数关系
- 理论证明，两层神经网络可以无限逼近任意连续函数

## 多层神经网络

可以拟合任何方程

此时ReLU算法更高效